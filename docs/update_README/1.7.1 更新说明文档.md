# 1.7.1 更新说明文档

**说明：本文档适用于PTA 1.7.0版本的更新升级，如果您项目中的版本过低，请按照之前的升级文档一步一步进行升级操作。**

## 本次升级所带来的优化内容

1、<a href="###资源更新" title="标题">资源更新</a>

2、<a href="###清理了不必要的素材和代码" title="标题">清理了不必要的素材和代码</a>

3、<a href="###身体拆分，根据不同的服装来选择不同级别的身体" title="标题">身体拆分，根据不同的服装来选择不同级别的身体</a>

4、<a href="###优化面部驱动功能为最新的CNN面驱" title="标题">优化面部驱动功能为最新的CNN面驱</a>

### 资源更新

+ 替换P2A文件夹（包含其中所有文件）
+ 替换FaceUnity-SDK-iOS文件夹（包含其中所有文件）
+ 替换items文件夹（包含其中所有文件）
+ 替换netconfig.json

### 清理了不必要的素材和代码

清理了大部分不需要的代码，并因为新的道具读取和绑定逻辑，代码逻辑有了较大改动，如果升级建议先替换以下文件

```
FUManager.h
FUManager.m
FUAvatar.h
FUavatar.m
FUShapeParamsMode.h
FUShapeParamsMode.m

```

然后根据报错在对其他文件进行对应的修改，具体修改参考demo

### 身体拆分，根据不同的服装来选择不同级别的身体

1.相关资源：（资源相关修改已经在‘资源更新’目录操作中完成，此处仅进行说明）

+ Resource文件夹：目前所有道具资源存放的文件，与工程文件在同一个目录，工程中仅引用在items文件夹下
+ Body相关bundle：删除了老的身体bundle，新的bundle位于items文件夹下body文件夹中
+ FUQitems.plist：新的道具类别目录，记录道具的类别和道具路径，位于items/Avatar_Q文件夹中

2.资源信息加载

道具信息加载，从`FUQItems.plist`文件中获取道具类别的信息，包括道具类别、道具类别的名称（中文名称，用于在编辑页显示并决定了类别的顺序）、该类别道具文件夹所在相对路径，然后从获取每个目录的config.json文件，获取道具配置信息

```objective-c
- (void)loadQtypeAvatarData
{
    NSMutableArray *itemInfoArray = [NSMutableArray arrayWithContentsOfFile:[[NSBundle mainBundle] pathForResource:@"FUQItems.plist" ofType:nil]];
    
    self.itemNameArray = [[NSMutableArray alloc]init];
    self.itemTypeArray = [[NSMutableArray alloc]init];
    self.itemsDict = [[NSMutableDictionary alloc]initWithCapacity:1];
    
    for (int i = 0; i < itemInfoArray.count; i++)
    {
        NSMutableArray *itemArray = [[NSMutableArray alloc]init];
        
        NSDictionary *dictItem = itemInfoArray[i];
        NSString *type = dictItem[@"type"];
        NSArray *paths = dictItem[@"path"];
        
        [self.itemTypeArray addObject:type];
        [self.itemNameArray addObject:dictItem[@"name"]];
        
        if (paths.count == 0)
        {
            continue;
        }
        
        for (int n = 0; n < paths.count; n++)
        {
            NSString *path = paths[n];
            NSString *configPath = [[NSBundle mainBundle].resourcePath stringByAppendingFormat:@"/Resource/%@/config.json",path];
            
            NSData *tmpData = [[NSString stringWithContentsOfFile:configPath encoding:NSUTF8StringEncoding error:nil] dataUsingEncoding:NSUTF8StringEncoding];
            
            NSMutableDictionary *dic = [NSJSONSerialization JSONObjectWithData:tmpData options:NSJSONReadingMutableContainers error:nil];
            NSArray *itemList = dic[@"list"];
            for (int j = 0; j < itemList.count; j++)
            {
                NSDictionary *item = itemList[j];
                
                if (itemArray.count > 0 && [item[@"icon"] isEqualToString:@"none"])
                {
                    continue;
                }
                
                FUItemModel *itemModel = [[FUItemModel alloc]init];
                itemModel.type = type;
                itemModel.path = [[NSBundle mainBundle].resourcePath stringByAppendingFormat:@"/Resource/%@",path];
                
                [item enumerateKeysAndObjectsUsingBlock:^(id  _Nonnull key, id  _Nonnull obj, BOOL * _Nonnull stop) {
                    [itemModel setValue:obj forKey:key];
                }];
                
                [itemArray addObject:itemModel];
            }
        }
        
        [self.itemsDict setObject:itemArray forKey:type];
    }
    
    [self loadColorList];
    [self loadMeshPoints];
    [self loadShapeList];
    
    [self loadAvatarList];
}

```

道具信息`FUItemModel`

```
@property (nonatomic, strong) NSString *type; //道具类别
@property (nonatomic, strong) NSString *path; //道具目录所在路径
@property (nonatomic, strong) NSString *name; //道具名称
@property (nonatomic, strong) NSString *bundle; //道具bundle的相对路径
@property (nonatomic, strong) NSString *icon;  //道具icon的相对路径
@property (nonatomic, strong) NSArray *label;
@property (nonatomic, strong) NSNumber *gender;
@property (nonatomic, strong) NSNumber *gender_match; //适合性别
@property (nonatomic, strong) NSNumber *body_match_level; //所需身体登记
@property (nonatomic, strong) NSMutableDictionary *shapeDict;  //脸部点位信息
```



2.绑定道具

传入道具的model绑定对应的道具

```
/// 绑定道具
/// @param model 道具相关信息
- (void)bindItemWithModel:(FUItemModel *)model
{
    FUAvatar *avatar = [FUManager shareInstance].currentAvatars.firstObject;
    BOOL undoOrRedo = [FUAvatarEditManager sharedInstance].undo||[FUAvatarEditManager sharedInstance].redo;
    
    if ([model.type isEqualToString:TAG_FU_ITEM_HAIR])
    {
        [avatar bindHairWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_FACE])
    {
        if ([model.name isEqualToString:@"捏脸"]&&!undoOrRedo)
        {
            self.shapeModeKey = [model.type stringByAppendingString:@"_front"];
            [[NSNotificationCenter defaultCenter] postNotificationName:FUEnterNileLianNot object:nil];
            return;
        }
        else
        {
            [avatar configFacepupParamWithDict:model.shapeDict];
        }
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_MOUTH])
    {
        if ([model.name isEqualToString:@"捏脸"]&&!undoOrRedo)
        {
            self.shapeModeKey = [model.type stringByAppendingString:@"_front"];
            [[NSNotificationCenter defaultCenter] postNotificationName:FUEnterNileLianNot object:nil];
            return;
        }
        else
        {
            [avatar configFacepupParamWithDict:model.shapeDict];
        }
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_EYE])
    {
        if ([model.name isEqualToString:@"捏脸"]&&!undoOrRedo)
        {
            self.shapeModeKey = [model.type stringByAppendingString:@"_front"];
            [[NSNotificationCenter defaultCenter] postNotificationName:FUEnterNileLianNot object:nil];
            return;
        }
        else
        {
            [avatar configFacepupParamWithDict:model.shapeDict];
        }
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_NOSE])
    {
        if ([model.name isEqualToString:@"捏脸"]&&!undoOrRedo)
        {
            self.shapeModeKey = [model.type stringByAppendingString:@"_front"];
            [[NSNotificationCenter defaultCenter] postNotificationName:FUEnterNileLianNot object:nil];
            return;
        }
        else
        {
            [avatar configFacepupParamWithDict:model.shapeDict];
        }
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_CLOTH])
    {
        self.isBindCloths = YES;
        [avatar bindClothWithItemModel:model];
        
        [self.selectedItemIndexDict setObject:@(0) forKey:TAG_FU_ITEM_UPPER];
        [self.selectedItemIndexDict setObject:@(0) forKey:TAG_FU_ITEM_LOWER];
        
        self.isBindCloths = NO;
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_UPPER])
    {
        self.isBindCloths = YES;
        if (avatar.clothType == FUAvataClothTypeSuit)
        {
            FUItemModel *lowerModel = [FUManager shareInstance].itemsDict[TAG_FU_ITEM_LOWER][1];
            [avatar bindLowerWithItemModel:lowerModel];
            [self.selectedItemIndexDict setObject:@(0) forKey:TAG_FU_ITEM_CLOTH];
            [self.selectedItemIndexDict setObject:@(1) forKey:TAG_FU_ITEM_LOWER];
        }
        [avatar bindUpperWithItemModel:model];
        self.isBindCloths = NO;
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_LOWER])
    {
        self.isBindCloths = YES;
        if (avatar.clothType == FUAvataClothTypeSuit)
        {
            FUItemModel *upperModel = [FUManager shareInstance].itemsDict[TAG_FU_ITEM_UPPER][1];
            [avatar bindUpperWithItemModel:upperModel];
            [self.selectedItemIndexDict setObject:@(0) forKey:TAG_FU_ITEM_CLOTH];
            [self.selectedItemIndexDict setObject:@(1) forKey:TAG_FU_ITEM_UPPER];
        }
        [avatar bindLowerWithItemModel:model];
        self.isBindCloths = NO;
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_SHOES])
    {
        [avatar bindShoesWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_HAT])
    {
        [avatar bindHatWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_EYELASH])
    {
        [avatar bindEyeLashWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_EYEBROW])
    {
        [avatar bindEyebrowWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_BEARD])
    {
        [avatar bindBeardWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_GLASSES])
    {
        [avatar bindGlassesWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_EYESHADOW])
    {
        [avatar bindEyeShadowWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_EYELINER])
    {
        [avatar bindEyeLinerWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_PUPIL])
    {
        [avatar bindPupilWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_FACEMAKEUP])
    {
        [avatar bindFaceMakeupWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_LIPGLOSS])
    {
        [avatar bindLipGlossWithItemModel:model];
    }
    else if ([model.type isEqualToString:TAG_FU_ITEM_DECORATION])
    {
        [avatar bindDecorationWithItemModel:model];
    }
    
    //设置undo堆栈
    if (!undoOrRedo)
    {
        NSMutableDictionary *editDict = [[NSMutableDictionary alloc]init];
        
        editDict[@"oldConfig"] = [avatar valueForKey:model.type];
        editDict[@"currentConfig"] = model;
        
        [[FUAvatarEditManager sharedInstance]push:editDict];
    }
    
    [FUAvatarEditManager sharedInstance].undo = NO;
    [FUAvatarEditManager sharedInstance].redo = NO;

    //设置选中索引
    NSArray *array = self.itemsDict[model.type];
    NSInteger index = [array containsObject:model]?[array indexOfObject:model]:0;

    [self.selectedItemIndexDict setObject:@(index) forKey:model.type];
    
    //修改模型信息的参数
    [avatar setValue:model forKey:model.type];
}

```

通过model判断所需要的身体bundle

```objective-c
- (void)bindClothWithItemModel:(FUItemModel *)model
{
    NSString *filepath = [NSString stringWithFormat:@"%@/%@",model.path,model.bundle];
    
    NSString *bodyFilepath = [self getBodyFilePathWithModel:model];
    
    dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
    [self bindItemWithType:FUItemTypeBody filePath:bodyFilepath];
    [self destroyItemWithType:FUItemTypeUpper];
    [self destroyItemWithType:FUItemTypeLower];
    [self bindItemWithType:FUItemTypeClothes filePath:filepath];
    self.clothType = FUAvataClothTypeSuit;
    dispatch_semaphore_signal(signal);
}


- (NSString *)getBodyFilePathWithModel:(FUItemModel *)model
{
    NSString *bodyFilepath = @"midBody";
    bodyFilepath = [bodyFilepath stringByAppendingFormat:@"_%@",[model.gender integerValue]>0?@"female":@"male"];
    bodyFilepath = [bodyFilepath stringByAppendingFormat:@"%zi.bundle",[model.body_match_level integerValue]];
    bodyFilepath = [[NSBundle mainBundle]pathForResource:bodyFilepath ofType:nil];
    
    return bodyFilepath;
}
```



### 优化面部驱动功能为最新的CNN面驱

1.导入face_capture.bundle

2.方法修改

`FUManager.h`中增加属性

```
@property void* faceCapture ;
@property BOOL useFaceCapure;
@property BOOL isFaceCaptureEnabled;
@property (nonatomic, strong) FURotatedImage *rotatedImageManager;
```

`FUManage.m`的`init`方法中初始化面驱功能

```
self.rotatedImageManager = [[FURotatedImage alloc]init];
[self initFaceCapture];
[self bindFaceCaptureToController];
[self useFaceCapure:YES];
```

相关方法实现

```
/// 初始化脸部识别
- (void)initFaceCapture
{
    NSData *data = [NSData dataWithContentsOfFile:[[NSBundle mainBundle] pathForResource:@"face_capture.bundle" ofType:nil]];
    self.faceCapture = [FURenderer faceCaptureCreate:data.bytes size:(int)data.length];
}

/// 绑定脸部识别到Controller
- (void)bindFaceCaptureToController
{
    [FURenderer itemSetParamu64:self.defalutQController  withName:@"register_face_capture_manager"  value:(unsigned long long)self.faceCapture];
    [FURenderer itemSetParam:self.defalutQController  withName:@"register_face_capture_face_id"  value:@(0.0)];
}

/// 重置脸部识别，切换摄像头时使用
- (void)faceCapureReset
{
    dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
    if (self.faceCapture)
    {
        [FURenderer faceCaptureReset:self.faceCapture];
        dispatch_semaphore_signal(self->signal);
    }
}

/// 销毁脸部识别
- (void)destroyFaceCapture
{
    [FURenderer faceCaptureDestory:self.faceCapture];
    self.faceCapture = nil;
}

/// 是否使用新的人脸驱动模式
/// @param isUse YES为使用，NO为不使用
- (void)useFaceCapure:(BOOL)isUse
{
    self.useFaceCapure = isUse;
    [FURenderer itemSetParam:self.defalutQController withName:@"is_close_dde" value:@(self.useFaceCapure?1.0:0.0)];
    //需要与初值相反
    self.isFaceCaptureEnabled = YES;
    [self enableFaceCapture:!self.isFaceCaptureEnabled];
}

- (void)enableFaceCapture:(BOOL)isEnable
{
    if(self.isFaceCaptureEnabled != isEnable)
    {
        self.isFaceCaptureEnabled = isEnable;
        [FURenderer itemSetParam:self.defalutQController withName:@"close_face_capture" value:@(isEnable?0.0:1.0)];
    }
}

- (void)runFaceCapture:(void*)imagePtr imageFormat:(FUFormat)imageFormat width:(int)width height:(int)height rotateMode:(int)rotateMode
{
    [FURenderer faceCaptureProcessFrame:self.faceCapture inPtr:imagePtr inFormat:imageFormat w:width h:height rotationMode:rotateMode];
}

/// 获取脸部识别识别到的人脸数量
- (int)getFaceCaptureFaceNum
{
    return [FURenderer faceCaptureGetResultFaceNum:self.faceCapture];
}

- (int)getFaceCaptureFaceID:(int)faceIdx
{
    return [FURenderer faceCaptureGetResultFaceID:self.faceCapture faceN:faceIdx];
}

/// 是否识别到人脸
- (int)faceCaptureGetResultIsFace
{
    int num = [FURenderer faceCaptureGetResultIsFace:self.faceCapture faceN:0];
    return num;
}

- (BOOL)isFaceCaptureFace:(int)faceIdx
{
    return [FURenderer faceCaptureGetResultIsFace:self.faceCapture faceN:faceIdx];
}

- (BOOL)getFaceCaptureResult:(FUAvatarInfo*)info faceIdx:(int)faceIdx
{
    if(nil == info)
    {
        return NO;
    }
    [info init];
    
    if(YES == self.useFaceCapure)
    {
        info->isValid= [self isFaceCaptureFace:faceIdx]?1:0;
        if(info->isValid)
        {
#define Get(Func, dst) [FURenderer faceCaptureGetResult##Func:self.faceCapture faceN:faceIdx buffer:info->dst length:sizeof(info->dst)/sizeof(info->dst[0])]
            Get(Landmarks, landmarks);
            Get(Identity, identity);
            Get(Expression, expression);
            Get(Rotation, rotation);
            Get(Translation, translation);
#undef Get
        }
    }
    else
    {
        info->isValid=[FURenderer isTracking];
        if(info->isValid)
        {
#define Get(_name, dst) [FURenderer getFaceInfo:faceIdx name:_name pret:info->dst number:sizeof(info->dst)/sizeof(info->dst[0])]
            Get(@"expression_aligned", expression);
            Get(@"translation_aligned", translation);
            Get(@"rotation_aligned", rotation);
            Get(@"rotation_mode", rotationMode);
            Get(@"pupil_pos", pupilPos);
            Get(@"landmarks", landmarks);
#undef Get
        }
    }
    info->info.is_valid=info->isValid;
    return info->isValid>0;
}

- (BOOL)GetLandmarks:(float*)buff length:(int)length faceIdx:(int)faceIdx
{
    if(YES == self.useFaceCapure)
    {
        if([self getFaceCaptureFaceNum]>0 && [self isFaceCaptureFace:faceIdx])
        {
            [FURenderer faceCaptureGetResultLandmarks:self.faceCapture faceN:faceIdx buffer:buff length:length];
        }
        else
        {
            return NO;
        }
    }
    else
    {
        if([FURenderer isTracking])
        {
            [FURenderer getFaceInfo:faceIdx name:@"landmarks" pret:buff number:length];
        }
        else
        {
            return NO;
        }
    }
    return YES;
}

- (void)trackFace:(CVPixelBufferRef)pixelBuffer
{
    CVPixelBufferLockBaseAddress(pixelBuffer, 0) ;
    void *baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) ;
    int height = (int)CVPixelBufferGetHeight(pixelBuffer) ;
    int stride = (int)CVPixelBufferGetBytesPerRow(pixelBuffer) ;
    if(YES == self.useFaceCapure)
    {
        [self runFaceCapture:baseAddress imageFormat:FU_FORMAT_BGRA_BUFFER width:stride/4 height:height rotateMode:0];
    }
    else
    {
        [FURenderer trackFaceWithTongue:FU_FORMAT_BGRA_BUFFER inputData:baseAddress width:stride/4 height:height];
    }
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0) ;
}

- (FUAvatarInfo*)GetAvatarInfo:(CVPixelBufferRef)pixelBuffer renderMode:(FURenderMode)renderMode
{
    FUAvatarInfo* info=[[FUAvatarInfo alloc] init];
    if (renderMode == FURenderPreviewMode)
    {
        [self trackFace:pixelBuffer];
        [self getFaceCaptureResult:info faceIdx:0];
    }
    [self enableFaceCapture:(renderMode == FURenderPreviewMode)];
    return info;
}
```

修改相关图像处理方法

```
/**
 检测人脸接口
 
 @param sampleBuffer  图像数据
 @return              图像数据
 */
- (CVPixelBufferRef)trackFaceWithBuffer:(CMSampleBufferRef)sampleBuffer CurrentlLightingValue:(float *)currntLightingValue
{
    dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
    CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) ;
    [self trackFace:pixelBuffer];
    
    if (CGSizeEqualToSize(frameSize, CGSizeZero))
    {
        CVPixelBufferLockBaseAddress(pixelBuffer, 0) ;
        int height = (int)CVPixelBufferGetHeight(pixelBuffer) ;
        int width  = (int)CVPixelBufferGetWidth(pixelBuffer) ;
        frameSize = CGSizeMake(width, height) ;
        CVPixelBufferUnlockBaseAddress(pixelBuffer,0);
    }
    
    CFDictionaryRef metadataDict = CMCopyDictionaryOfAttachments(NULL,sampleBuffer, kCMAttachmentMode_ShouldPropagate);
    NSDictionary *metadata = [[NSMutableDictionary alloc] initWithDictionary:(__bridge NSDictionary*)metadataDict];
    CFRelease(metadataDict);
    NSDictionary *exifMetadata = [[metadata objectForKey:(NSString *)kCGImagePropertyExifDictionary] mutableCopy];
    lightingValue = [[exifMetadata objectForKey:(NSString *)kCGImagePropertyExifBrightnessValue] floatValue];
    if(currntLightingValue)
    {
        *currntLightingValue = lightingValue;
    }
    dispatch_semaphore_signal(signal);
    return pixelBuffer ;
}


static int frameId = 0 ;
/**
 Avatar 处理接口
 
 @param pixelBuffer 图像数据
 @param renderMode  render 模式
 @param landmarks   landmarks 数组
 @return            处理之后的图像
 */
- (CVPixelBufferRef)renderP2AItemWithPixelBuffer:(CVPixelBufferRef)pixelBuffer RenderMode:(FURenderMode)renderMode Landmarks:(float *)landmarks LandmarksLength:(int)landmarksLength
{
    dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
    
    FUAvatarInfo* info;
    if(self.useFaceCapure)
    {
        info = [[FUAvatarInfo alloc] init];
        [self enableFaceCapture:(FURenderPreviewMode==renderMode?YES:NO)];
        if(FURenderPreviewMode == renderMode)
        {
            [self trackFace:pixelBuffer];
            if(landmarks)
            {
                [self GetLandmarks:landmarks length:landmarksLength faceIdx:0];
            }
        }
    }
    else
    {
        info = [self GetAvatarInfo:pixelBuffer renderMode:renderMode];
        if(FURenderPreviewMode == renderMode)
        {
            if(landmarks)
            {
                memcpy(landmarks, info->landmarks, sizeof(info->landmarks));
            }
        }
    }
    
    int h = (int)CVPixelBufferGetHeight(renderTarget);
    int stride = (int)CVPixelBufferGetBytesPerRow(renderTarget);
    int w = stride/4;
    CVPixelBufferLockBaseAddress(renderTarget, 0);
    void* pod = (void *)CVPixelBufferGetBaseAddress(renderTarget);
    [[FURenderer shareRenderer] renderBundles:&info->info inFormat:FU_FORMAT_AVATAR_INFO outPtr:pod outFormat:FU_FORMAT_BGRA_BUFFER width:w height:h frameId:frameId++ items:mItems itemCount:sizeof(mItems)/sizeof(int)];
    
    [self rotateImage:pod inFormat:FU_FORMAT_BGRA_BUFFER w:w h:h rotationMode:FU_ROTATION_MODE_0 flipX:NO flipY:YES];
    
    memcpy(pod, self.rotatedImageManager.mData, w*h*4);
    CVPixelBufferUnlockBaseAddress(renderTarget, 0);
    
    dispatch_semaphore_signal(signal);
    
    return renderTarget ;
}


static int ARFilterID = 0 ;
/**
 AR 滤镜处理接口 同时返回捕捉到的脸部点位
 
 @param pixelBuffer 图像数据
 @return            处理之后的图像数据
 */
- (CVPixelBufferRef)renderARFilterItemWithBuffer:(CVPixelBufferRef)pixelBuffer Landmarks:(float *)landmarks LandmarksLength:(int)landmarksLength
{
    
    dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
    
    if(self.useFaceCapure)
    {
        [self trackFace:pixelBuffer];
        if(landmarks)
        {
            [self GetLandmarks:landmarks length:landmarksLength faceIdx:0];
        }
        [self enableFaceCapture:YES];
    }
    
    int h = (int)CVPixelBufferGetHeight(pixelBuffer);
    int stride = (int)CVPixelBufferGetBytesPerRow(pixelBuffer);
    int w = stride/4;
    CVPixelBufferLockBaseAddress(pixelBuffer, 0);
    void* pod0 = (void *)CVPixelBufferGetBaseAddress(pixelBuffer);
    [[FURenderer shareRenderer] renderBundles:pod0 inFormat:FU_FORMAT_BGRA_BUFFER outPtr:pod0 outFormat:FU_FORMAT_BGRA_BUFFER width:w height:h frameId:ARFilterID++ items:arItems itemCount:2];
    
    [self rotateImage:pod0 inFormat:FU_FORMAT_BGRA_BUFFER w:w h:h rotationMode:FU_ROTATION_MODE_0 flipX:NO flipY:YES];
    memcpy(pod0, self.rotatedImageManager.mData, w*h*4);
    
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);
    
    dispatch_semaphore_signal(signal);
    
    return pixelBuffer;
}
```



3.新面驱使用

首页`ViewController`中脸部追踪中使用，修改`-(**void**)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer`方法

```
static int frameIndex = 0;
CRender * viewRender;
-(void)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer
{
    
    if ([FUManager shareInstance].isBindCloths)
    {
        return;
    }
    
	dispatch_semaphore_wait(signal, DISPATCH_TIME_FOREVER);
	if (loadingBundles)
    {
		return;
	}

	frameIndex ++;
	
	CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);

	CVPixelBufferRef mirrored_pixel = [[FUManager shareInstance] dealTheFrontCameraPixelBuffer:pixelBuffer];
    const int landmarks_cnt = 314;
    float landmarks[landmarks_cnt] ;

    CVPixelBufferRef buffer = [[FUManager shareInstance] renderP2AItemWithPixelBuffer:mirrored_pixel RenderMode:renderMode Landmarks:landmarks LandmarksLength:landmarks_cnt];
    CGSize size = [UIScreen mainScreen].currentMode.size;
    
	[self.displayView displayPixelBuffer:buffer withLandmarks:nil count:0 Mirr:NO];
	switch (self.videoRecordState)
    {
		case Original:

			break;
		case Recording:
		{
			CVPixelBufferRef mirrorYBuffer = [_recordRender cutoutPixelBuffer:buffer WithRect:CGRectMake(0, 0, size.width-200,size.height-200)];
			[[FUP2AHelper shareInstance] recordBufferWithType:FUP2AHelperRecordTypeVoicedVideo buffer:mirrorYBuffer sampleBuffer:sampleBuffer Completion:^(CFAbsoluteTime duration) {
				NSLog(@"当前帧返回时长-------------%f",duration);
			}];
		}
			break;
		case Completed:
		{

			[[FUP2AHelper shareInstance] stopRecordWithType:FUP2AHelperRecordTypeVoicedVideo TimeCompletion:^(NSString *retPath,CFAbsoluteTime duration) {
				dispatch_async(dispatch_get_main_queue(), ^{
					NSLog(@"视频位置是------------------------%@",retPath);
					NSLog(@"录制时长是------------------%f",duration);
					[self saveRecordedVideo:retPath];
				});
			}];


			self.videoRecordState = Original;
		}
			break;

		default:
			break;
	}
	
	if (renderMode == FURenderPreviewMode)
    {
		[self.preView displayPixelBuffer:pixelBuffer withLandmarks:landmarks count:150 Mirr:YES];
	}
	CVPixelBufferRelease(mirrored_pixel);
	dispatch_semaphore_signal(signal);
}
```



拍照生成形象中应用cnn，修改`-(void)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer`

```

static int frameID = 0;
-(void)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer {
	
	__weak typeof(self)weakSelf = self ;
	
	// CVPixelBufferRef buffer =  CMSampleBufferGetImageBuffer(sampleBuffer) ;
	float lightValue = 0;
	CVPixelBufferRef buffer = [[FUManager shareInstance] trackFaceWithBuffer:sampleBuffer CurrentlLightingValue:&lightValue];
	dispatch_async(dispatch_get_main_queue(), ^{
	});
	int h = (int)CVPixelBufferGetHeight(buffer);
	int w = (int)CVPixelBufferGetWidth(buffer);
	if (self.camera.isFrontCamera) {
		CVPixelBufferRef mirrorBuffer = [[CRender shareRenderer] cutoutPixelBufferInXMirror:buffer WithRect:CGRectMake(0, 0, w, h)];
		[self.displayView displayPixelBuffer:mirrorBuffer withLandmarks:nil count:0 Mirr:NO];
		
	}else{
		[self.displayView displayPixelBuffer:buffer withLandmarks:nil count:0 Mirr:NO];
		
	}
	frameID ++ ;
	if (frameID % 15 == 0) {
		
		NSString *message  = [[FUManager shareInstance] photoDetectionAction];
		
		dispatch_async(dispatch_get_main_queue(), ^{
			weakSelf.messageLabel.text = message ;
		});
	}
	
	if (takePhoto) {
		takePhoto = NO ;
		
		CGRect faceRect = [[FUManager shareInstance] getFaceRect];
        int faceNum = [[FUManager shareInstance] faceCaptureGetResultIsFace];
        if (CGSizeEqualToSize(faceRect.size, CGSizeZero)|| faceNum != 1) {
			self.currentType = FUCurrentViewTypeNone ;
			[self downloadErrorWithMessage:@"面部识别失败，请重新尝试。"];
			self.iconImage = nil ;
			self.selectedImage = nil ;
			return ;
		}
		
		CVPixelBufferRef imageBuffer;
		
		
		
		if (self.camera.isFrontCamera) {
			imageBuffer = [[CRender shareRenderer] cutoutPixelBufferInXMirror:buffer WithRect:faceRect];
			self.iconImage = [self.camera getSquareImageFromBuffer:imageBuffer];
			self.selectedImage = [[FUP2AHelper shareInstance] createImageWithBuffer:buffer mirr:YES];
		}else{
			imageBuffer = [[CRender shareRenderer] cutoutPixelBuffer:buffer WithRect:faceRect];
			self.iconImage = [self.camera getSquareImageFromBuffer:imageBuffer];
			self.selectedImage = [[FUP2AHelper shareInstance] createImageWithBuffer:buffer mirr:NO];
		}
		
		dispatch_async(dispatch_get_main_queue(), ^{
			
			[weakSelf.camera stopCapture];
			
			weakSelf.currentType = FUCurrentViewTypePreparingCreat ;
			
			weakSelf.photoImageView.image = weakSelf.selectedImage ;
			weakSelf.photoImageView.hidden = NO ;
			weakSelf.imageView.hidden = YES ;
			weakSelf.messageLabel.hidden = YES ;
			weakSelf.photoBtn.hidden = YES ;
			weakSelf.libraryBtn.hidden = YES ;
			weakSelf.switchBtn.hidden = YES ;
			[weakSelf selectedGenderAndLoading];
		});
	}
}
```

并修改`FUManage.h`中的拍照检测方法

```
static float DetectionAngle = 20.0;
static float CenterScale = 0.3;
/**
 拍摄检测
 
 @return 检测结果
 */
- (NSString *)photoDetectionAction
{
    // 1、保证单人脸输入
    int faceNum = [self faceCaptureGetResultIsFace];
    if (faceNum != 1)
    {
        return @" 请保持1个人输入  ";
    }

    // 2、保证正脸
    float rotation[4];
    [FURenderer getFaceInfo:0 name:@"rotation" pret:rotation number:4];
    
    float xAngle = atanf(2 * (rotation[3] * rotation[0] + rotation[1] * rotation[2]) / (1 - 2 * (rotation[0] * rotation[0] + rotation[1] * rotation[1]))) * 180 / M_PI;
    float yAngle = asinf(2 * (rotation[1] * rotation[3] - rotation[0] * rotation[2])) * 180 / M_PI;
    float zAngle = atanf(2 * (rotation[3] * rotation[2] + rotation[0] * rotation[1]) / (1 - 2 * (rotation[1] * rotation[1] + rotation[2] * rotation[2]))) * 180 / M_PI;
    
    if (xAngle < -DetectionAngle || xAngle > DetectionAngle
        || yAngle < -DetectionAngle || yAngle > DetectionAngle
        || zAngle < -DetectionAngle || zAngle > DetectionAngle)
    {
        return @" 识别失败，需要人物正脸完整出镜哦~  ";
    }
    
    // 3、保证人脸在中心区域
    CGPoint faceCenter = [self getFaceCenterInFrameSize:frameSize];
    
    if (faceCenter.x < 0.5 - CenterScale / 2.0 || faceCenter.x > 0.5 + CenterScale / 2.0
        || faceCenter.y < 0.4 - CenterScale / 2.0 || faceCenter.y > 0.4 + CenterScale / 2.0)
    {
        return @" 请将人脸对准虚线框  ";
    }
    
    // 4、夸张表情
    float expression[46];
    [FURenderer getFaceInfo:0 name:@"expression" pret:expression number:46];
    
    for (int i = 0; i < 46; i ++)
    {
        if (expression[i] > 1)
        {
            return @" 请保持面部无夸张表情  ";
        }
    }
    
    // 5、光照均匀
    // 6、光照充足
    if (lightingValue < -1.0) {
        return @" 光线不充足  ";
    }
    
    return @" 完美  ";
}

/**
 获取人脸矩形框
 
 @return 人脸矩形框
 */
- (CGRect)getFaceRect
{
    float faceRect[4];
    int ret = [FURenderer faceCaptureGetResultFaceBbox:self.faceCapture faceN:0 buffer:faceRect length:4];
    if (!ret)
    {
        return CGRectZero;
    }
    // 计算出中心点的坐标值
    CGFloat centerX = (faceRect[0] + faceRect[2]) * 0.5;
    CGFloat centerY = (faceRect[1] + faceRect[3]) * 0.5;
    
    // 将坐标系转换成以左上角为原点的坐标系
    centerX = frameSize.width - centerX;
    centerY = frameSize.height - centerY;
    
    CGRect rect = CGRectZero;
    if (frameSize.width < frameSize.height)
    {
        CGFloat w = frameSize.width;
        rect.size = CGSizeMake(w, w);
        rect.origin = CGPointMake(0, centerY - w/2.0);
    }
    else
    {
        CGFloat w = frameSize.height;
        rect.size = CGSizeMake(w, w);
        rect.origin = CGPointMake(centerX - w / 2.0, 0);
    }
    
    CGPoint origin = rect.origin;
    if (origin.x < 0)
    {
        origin.x = 0;
    }
    if (origin.y < 0)
    {
        origin.y = 0;
    }
    rect.origin = origin;
    
    return rect;
}

/**获取图像中人脸中心点*/
- (CGPoint)getFaceCenterInFrameSize:(CGSize)frameSize
{
    static CGPoint preCenter;
    static dispatch_once_t onceToken;
    dispatch_once(&onceToken, ^{
        preCenter = CGPointMake(0.49, 0.5);
    });
    
    // 获取人脸矩形框，坐标系原点为图像右下角，float数组为矩形框右下角及左上角两个点的x,y坐标（前两位为右下角的x,y信息，后两位为左上角的x,y信息）
    float faceRect[4];
    int ret = [FURenderer faceCaptureGetResultFaceBbox:self.faceCapture faceN:0 buffer:faceRect length:4];
    
    if (ret == 0)
    {
        return preCenter;
    }
    
    // 计算出中心点的坐标值
    CGFloat centerX = (faceRect[0] + faceRect[2]) * 0.5;
    CGFloat centerY = (faceRect[1] + faceRect[3]) * 0.5;
    
    // 将坐标系转换成以左上角为原点的坐标系
    centerX = frameSize.width - centerX;
    centerX = centerX / frameSize.width;
    
    centerY = frameSize.height - centerY;
    centerY = centerY / frameSize.height;
    
    CGPoint center = CGPointMake(centerX, centerY);
    
    preCenter = center;
    
    return center;
}
```

驱动功能模块中应用cnn，修改`-(void)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer`

```
-(void)didOutputVideoSampleBuffer:(CMSampleBufferRef)sampleBuffer {
	
	CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) ;
    const int landmarks_cnt = 314;
    float landmarks[landmarks_cnt] ;
    if (self.camera.isFrontCamera)
    {
		CVPixelBufferRef mirrored_pixel = [[FUManager shareInstance] dealTheFrontCameraPixelBuffer:pixelBuffer];
		[[FUManager shareInstance] renderARFilterItemWithBuffer:mirrored_pixel Landmarks:landmarks LandmarksLength:landmarks_cnt];
		
		[self.renderView displayPixelBuffer:mirrored_pixel withLandmarks:nil count:0 Mirr:NO];
		CVPixelBufferRelease(mirrored_pixel);
	}
    else
    {
		[[FURenderer shareRenderer] setInputCameraMatrix:0 flip_y:0 rotate_mode:0];
		[[FUManager shareInstance] renderARFilterItemWithBuffer:pixelBuffer Landmarks:landmarks LandmarksLength:landmarks_cnt];
		[self.renderView displayPixelBuffer:pixelBuffer withLandmarks:nil count:0 Mirr:NO];
	}
}
```



