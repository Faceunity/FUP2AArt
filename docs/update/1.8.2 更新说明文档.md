# 1.8.2 更新说明文档

**说明：本文档适用于PTA 1.8.1版本的更新升级，如果您项目中的版本过低，请按照之前的升级文档一步一步进行升级操作。**

## 本次升级所带来的优化内容

##### 1.优化渲染性能，将nama.a改为cnama.a

##### 2.优化驱动算法模型

##### 3.更新接口域名

##### 4.将使用controller.bundle 改为 使用 controller_cpp.bundle

## 1、资源更新

替换/新增文件夹中的资源 

1.更新 FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/controller.bundle ===> FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/controller_cpp.bundle;


2.更新 FUP2A/Faceunity/FaceUnity-SDK-iOS/libnama.a ===> FUP2A/Faceunity/FaceUnity-SDK-iOS/libCNamaSDK.a


3.更新 FUP2A/Faceunity/FaceUnity-SDK-iOS/libfuai.a 文件;


4.更新 FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/face_processor_capture.bundle ===> FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/ai_face_processor.bundle;


5.更新 FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/human3d.bundle  ===> FUP2A/Faceunity/FaceUnity-SDK-iOS/Resources/ai_human_processor.bundle;


## 1、代码更新
1.加载 controller_cpp.bundle 代替 controller.bundle
```
    //加载controller
    NSString *controllerPath = [[NSBundle mainBundle] pathForResource:@"controller_cpp" ofType:@"bundle"];
    self.defalutQController = [FURenderer itemWithContentsOfFile:controllerPath];
```
2.更新加载面驱跟身体驱动的方法

```
/// 初始化脸部识别
- (void)initFaceCapture
{
    NSData *data = [NSData dataWithContentsOfFile:[[NSBundle mainBundle] pathForResource:@"ai_face_processor.bundle" ofType:nil]];
//    fuLoadAIModelFromPackage((void *)data.bytes , (int) data.length, FUAITYPE_FACEPROCESSOR);
    [FURenderer loadAIModelFromPackage:(void *)data.bytes size:(int) data.length aitype:FUAITYPE_FACEPROCESSOR];
}

- (void)initHuman3D
{
    NSData *human3dData = [NSData dataWithContentsOfFile:[[NSBundle mainBundle] pathForResource:@"ai_human_processor.bundle" ofType:nil]];
//    _human3dPtr = [FURenderer create3DBodyTracker:(void*)human3dData.bytes size:(int)human3dData.length];
    [FURenderer loadAIModelFromPackage:(void *)human3dData.bytes size:(int)human3dData.length aitype:FUAITYPE_HUMAN_PROCESSOR];
}

- (void)destroyFaceCapture
{
    fuReleaseAIModel(FUAITYPE_FACEPROCESSOR);
}

- (void)enableFaceCapture:(int)enable
{
   fuItemSetParamd(self.defalutQController, "enable_face_processor", enable);
}
```

3.修改人脸识别及获取人脸信息的相关方法

```

/**
 拍摄检测
 
 @return 检测结果
 */
- (NSString *)photoDetectionAction
{
    // 1、保证单人脸输入
    int faceNum = [FURenderer isTracking];
    if (faceNum != 1)
    {
        return @" 请保持1个人输入  ";
    }

    // 2、保证正脸
    float rotation[4];
    [FURenderer getFaceInfo:0 name:@"rotation" pret:rotation number:4];

    float xAngle = atanf(2 * (rotation[3] * rotation[0] + rotation[1] * rotation[2]) / (1 - 2 * (rotation[0] * rotation[0] + rotation[1] * rotation[1]))) * 180 / M_PI;
    float yAngle = asinf(2 * (rotation[1] * rotation[3] - rotation[0] * rotation[2])) * 180 / M_PI;
    float zAngle = atanf(2 * (rotation[3] * rotation[2] + rotation[0] * rotation[1]) / (1 - 2 * (rotation[1] * rotation[1] + rotation[2] * rotation[2]))) * 180 / M_PI;

    if (xAngle < -DetectionAngle || xAngle > DetectionAngle
        || yAngle < -DetectionAngle || yAngle > DetectionAngle
        || zAngle < -DetectionAngle || zAngle > DetectionAngle)
    {
        return @" 识别失败，需要人物正脸完整出镜哦~  ";
    }

    // 3、保证人脸在中心区域
    CGPoint faceCenter = [self getFaceCenterInFrameSize:frameSize];

    if (faceCenter.x < 0.5 - CenterScale / 2.0 || faceCenter.x > 0.5 + CenterScale / 2.0
        || faceCenter.y < 0.4 - CenterScale / 2.0 || faceCenter.y > 0.4 + CenterScale / 2.0)
    {
        return @" 请将人脸对准虚线框  ";
    }

    // 4、夸张表情
    float expression[46];
    [FURenderer getFaceInfo:0 name:@"expression" pret:expression number:46];
    for (int i = 0; i < 46; i ++)
    {
        if (expression[i] > 0.5)
        {
            return @" 请保持面部无夸张表情  ";
        }
    }

    // 5、光照均匀
    // 6、光照充足
    if (lightingValue < -1.0) {
        return @" 光线不充足  ";
    }
    
    return @" 完美  ";
}

/**
 获取人脸矩形框
 
 @return 人脸矩形框
 */
- (CGRect)getFaceRect
{
    float faceRect[4];
    int ret = [FURenderer getFaceInfo:0 name:@"face_rect" pret:faceRect number:4];
    if (!ret)
    {
        return CGRectZero;
    }
    // 计算出中心点的坐标值
    CGFloat centerX = (faceRect[0] + faceRect[2]) * 0.5;
    CGFloat centerY = (faceRect[1] + faceRect[3]) * 0.5;
    
    // 将坐标系转换成以左上角为原点的坐标系
    centerX = frameSize.width - centerX;
    centerY = frameSize.height - centerY;
    
    CGRect rect = CGRectZero;
    if (frameSize.width < frameSize.height)
    {
        CGFloat w = frameSize.width;
        rect.size = CGSizeMake(w, w);
        rect.origin = CGPointMake(0, centerY - w/2.0);
    }
    else
    {
        CGFloat w = frameSize.height;
        rect.size = CGSizeMake(w, w);
        rect.origin = CGPointMake(centerX - w / 2.0, 0);
    }
    
    CGPoint origin = rect.origin;
    if (origin.x < 0)
    {
        origin.x = 0;
    }
    if (origin.y < 0)
    {
        origin.y = 0;
    }
    rect.origin = origin;
    
    return rect;
}

/**获取图像中人脸中心点*/
- (CGPoint)getFaceCenterInFrameSize:(CGSize)frameSize
{
    static CGPoint preCenter;
    static dispatch_once_t onceToken;
    dispatch_once(&onceToken, ^{
        preCenter = CGPointMake(0.49, 0.5);
    });
    
    // 获取人脸矩形框，坐标系原点为图像右下角，float数组为矩形框右下角及左上角两个点的x,y坐标（前两位为右下角的x,y信息，后两位为左上角的x,y信息）
    float faceRect[4];
    int ret = [FURenderer getFaceInfo:0 name:@"face_rect" pret:faceRect number:4];
//    [FURenderer faceCaptureGetResultFaceBbox:self.faceCapture faceN:0 buffer:faceRect length:4];
    
    if (ret == 0)
    {
        return preCenter;
    }
    
    // 计算出中心点的坐标值
    CGFloat centerX = (faceRect[0] + faceRect[2]) * 0.5;
    CGFloat centerY = (faceRect[1] + faceRect[3]) * 0.5;
    
    // 将坐标系转换成以左上角为原点的坐标系
    centerX = frameSize.width - centerX;
    centerX = centerX / frameSize.width;
    
    centerY = frameSize.height - centerY;
    centerY = centerY / frameSize.height;
    
    CGPoint center = CGPointMake(centerX, centerY);
    
    preCenter = center;
    
    return center;
}



```



4.修改拍照时的渲染接口

```
/**
 检测人脸接口
 
 @param sampleBuffer  图像数据
 @return              图像数据
 */
- (CVPixelBufferRef)trackFaceWithBuffer:(CMSampleBufferRef)sampleBuffer CurrentlLightingValue:(float *)currntLightingValue
{
    dispatch_semaphore_wait(self.signal, DISPATCH_TIME_FOREVER);
    
    CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) ;
    
    CVPixelBufferLockBaseAddress(pixelBuffer, 0) ;
    
    void *baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) ;
    int height = (int)CVPixelBufferGetHeight(pixelBuffer) ;
    int width  = (int)CVPixelBufferGetWidth(pixelBuffer) ;
    
    [FURenderer trackFace:FU_FORMAT_BGRA_BUFFER inputData:baseAddress width:width height:height];
    
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0) ;
    
    if (CGSizeEqualToSize(frameSize, CGSizeZero)) {
        frameSize = CGSizeMake(width, height) ;
    }
    
    CFDictionaryRef metadataDict = CMCopyDictionaryOfAttachments(NULL,sampleBuffer, kCMAttachmentMode_ShouldPropagate);
    NSDictionary *metadata = [[NSMutableDictionary alloc] initWithDictionary:(__bridge NSDictionary*)metadataDict];
    CFRelease(metadataDict);
    NSDictionary *exifMetadata = [[metadata objectForKey:(NSString *)kCGImagePropertyExifDictionary] mutableCopy];
    lightingValue = [[exifMetadata objectForKey:(NSString *)kCGImagePropertyExifBrightnessValue] floatValue];
    *currntLightingValue = lightingValue;
    dispatch_semaphore_signal(self.signal);
    return pixelBuffer ;
}
```
5.修改编辑页和预览页的渲染接口

```
/**
 Avatar 处理接口
 
 @param pixelBuffer 图像数据
 @param renderMode  render 模式
 @param landmarks   landmarks 数组
 @return            处理之后的图像
 */
- (CVPixelBufferRef)renderP2AItemWithPixelBuffer:(CVPixelBufferRef)pixelBuffer RenderMode:(FURenderMode)renderMode Landmarks:(float *)landmarks LandmarksLength:(int)landmarksLength
{
	dispatch_semaphore_wait(self.signal, DISPATCH_TIME_FOREVER);
	
    int renderTarget_h = (int)CVPixelBufferGetHeight(renderTarget);
    int renderTarget_stride = (int)CVPixelBufferGetBytesPerRow(renderTarget);
    int renderTarget_w = renderTarget_stride/4;
//	 int w = 750;/
    CVPixelBufferLockBaseAddress(renderTarget, 0);
    void* renderTarget_pod = (void *)CVPixelBufferGetBaseAddress(renderTarget);
    CVPixelBufferLockBaseAddress(pixelBuffer, 0);
    void* pixelBuffer_pod = (void *)CVPixelBufferGetBaseAddress(pixelBuffer);
	
    int pixelBuffer_h = (int)CVPixelBufferGetHeight(pixelBuffer);
    int pixelBuffer_stride = (int)CVPixelBufferGetBytesPerRow(pixelBuffer);
    int pixelBuffer_w = pixelBuffer_stride/4;
	if (renderMode == FURenderPreviewMode)
	{
		[[FURenderer shareRenderer] renderBundles:pixelBuffer_pod inFormat:FU_FORMAT_BGRA_BUFFER outPtr:renderTarget_pod outFormat:FU_FORMAT_BGRA_BUFFER width:pixelBuffer_w height:pixelBuffer_h frameId:frameId ++ items:mItems itemCount:sizeof(mItems)/sizeof(int)];
	}else
	{
		
		float expression[56] = {0};
		float translation[3] = {0};
		float rotation[4] = {0};
		rotation[3] = 1;
		float rotation_mode[1] = {0};
		float pupil_pos[2] = {0};
		int is_valid = 0 ;
		
		
		TAvatarInfo info;
		info.p_translation = translation;
		info.p_rotation = rotation;
		info.p_expression = expression;
		info.rotation_mode = rotation_mode;
		info.pupil_pos = pupil_pos;
		info.is_valid = is_valid;
		
		[[FURenderer shareRenderer] renderBundles:&info inFormat:FU_FORMAT_AVATAR_INFO outPtr:renderTarget_pod outFormat:FU_FORMAT_BGRA_BUFFER width:renderTarget_w height:renderTarget_h frameId:frameId ++ items:mItems itemCount:sizeof(mItems)/sizeof(int)];
	}
	
	[FURenderer getFaceInfo:0 name:@"landmarks" pret:landmarks number:landmarksLength];
	[self rotateImage:renderTarget_pod inFormat:FU_FORMAT_BGRA_BUFFER w:renderTarget_w h:renderTarget_h rotationMode:FU_ROTATION_MODE_0 flipX:NO flipY:YES];
	memcpy(renderTarget_pod, self.rotatedImageManager.mData, renderTarget_w*renderTarget_h*4);
	CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);
	CVPixelBufferUnlockBaseAddress(renderTarget, 0);
	
	dispatch_semaphore_signal(self.signal);
	
	return renderTarget;
}
```

6.修改身体追踪界面的渲染接口
```
- (CVPixelBufferRef)renderBodyTrackWithBuffer:(CVPixelBufferRef)pixelBuffer ptr:(void *)human3dPtr RenderMode:(FURenderMode)renderMode Landmarks:(float *)landmarks LandmarksLength:(int)landmarksLength
{
    dispatch_semaphore_wait(self.signal, DISPATCH_TIME_FOREVER);
     
    int h = (int)CVPixelBufferGetHeight(pixelBuffer);
    int stride = (int)CVPixelBufferGetBytesPerRow(pixelBuffer);
    int w = stride/4;
    CVPixelBufferLockBaseAddress(pixelBuffer, 0);
    void* pod0 = (void *)CVPixelBufferGetBaseAddress(pixelBuffer);
    
    [[FURenderer shareRenderer] renderBundles:pod0 inFormat:FU_FORMAT_BGRA_BUFFER outPtr:pod0 outFormat:FU_FORMAT_BGRA_BUFFER width:w height:h frameId:frameId ++ items:mItems itemCount:sizeof(mItems)/sizeof(int)];
    
    [self rotateImage:pod0 inFormat:FU_FORMAT_BGRA_BUFFER w:w h:h rotationMode:FU_ROTATION_MODE_0 flipX:NO flipY:YES];
    memcpy(pod0, self.rotatedImageManager.mData, w*h*4);
    
    CVPixelBufferUnlockBaseAddress(pixelBuffer, 0);
    
    dispatch_semaphore_signal(self.signal);
    
    return pixelBuffer;
}
```

详细见 FUP2A/mode/FUManager.m 文件